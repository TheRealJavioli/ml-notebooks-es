{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIb7AW-J_Q0j"
   },
   "source": [
    "In this simple notebook we use a fully connected neural network to solve a previously seen problem in classification: the particle physics ID problem.\n",
    "\n",
    "It accompanies Chapter 8 of the book.\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wCi2a2GB_Q0m"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ty1aqrju_Q0m"
   },
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RELqQBID_Q0n"
   },
   "source": [
    "Tensorflow is a very commonly used library used in development of Deep Learning models. It is an open-source platform that was developed by Google. It supports programming in several languages, e.g. C++, Java, Python, and many others.\n",
    "\n",
    "Keras is a high-level API (Application Programming Interface) that is built on top of TensorFlow (or Theano, another Deep Learning library). It is Python-specific, and we can think of it as the equivalent of the sklearn library for neural network. It is less general, and less customizable, but it is very user-friendly and comparatively easier than TensorFlow. We will use keras with the tensorflow back-end."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fAX7Ddfd_Q0n"
   },
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "qN9_Zvz0_fE_",
    "outputId": "c0252c1f-70c3-4e03-fc12-6177ca1c1ca7"
   },
   "source": [
    "tf.__version__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lzdIblJi_Q0n"
   },
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "\n",
    "from keras.layers import Dropout #for regularization"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AddTCBp6_Q0o"
   },
   "source": [
    "We begin with the 4top vs ttbar problem, and we use the configuration where we added the features \"number of leptons\", \"number of jets\" etc. For reference, the optimal SVM achieved 94-95% accuracy. Note that those numbers had not been run through <b> nested </b> cross validation so they might be slightly optimistic. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GdeqbK8B_Q0o"
   },
   "source": [
    "X = pd.read_csv('../data/Features_lim_2.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U-Qxlpua_Q0p"
   },
   "source": [
    "y = np.genfromtxt('../data/Labels_lim_2.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLOOvTME_Q0q",
    "outputId": "608114b8-b821-474b-d810-eaf952119694"
   },
   "source": [
    "X.values.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo9nE9rL_Q0r"
   },
   "source": [
    "There is no \"built-in\" cross validation (or nested cross validation) process, so we would need to build it ourselves. For now, we can build three sets: train, validation (for parameter optimization), and test (for final evaluation). We should ideally build this as a cross-validation structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jGV2FTWj_Q0y"
   },
   "source": [
    "#Always shuffle first\n",
    "\n",
    "X,y = shuffle(X,y, random_state = 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XolI39tH_Q0y"
   },
   "source": [
    "X_train = X.values[:3000,:]\n",
    "y_train = y[:3000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rLRFgtDm_Q0y"
   },
   "source": [
    "X_val = X.values[3000:4000,:]\n",
    "y_val = y[3000:4000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nn046-as_Q0z"
   },
   "source": [
    "X_test = X.values[4000:,:]\n",
    "y_test = y[4000:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZmgrG6Y_Q0z",
    "outputId": "5c905f59-303d-4aa0-af50-4124f1412cdc"
   },
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6644VLi_Q0z"
   },
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws9Ko4wv_Q0z"
   },
   "source": [
    "Let's think about the model architecture.\n",
    "\n",
    "Our input layer has 24 neurons. \n",
    "\n",
    "Our output layer has one neuron (the output is the probability that the object belongs to the positive class). We could also set it up as two neurons (and have softmax as the final non-linearity), but this is redundant in a binary classification problem.\n",
    "\n",
    "We will add two hidden layers. Here I'm making their sizes = 20 (I should optimize this hyperparameter!). We can also reserve the possibility of adding a dropout layer after each one. The dropout fraction should also be optimized through CV.\n",
    "\n",
    "Other decisions that we have to make are: which nonlinearities we use (for now: ReLU for hidden layers, sigmoid for the final one), which optimizer we use (Adam), which starting learning rate we adopt (here 0.001, but again this should be decided through CV), the number of epochs (e.g. 100; we can plot quantities of interest to check that we have enough), the batch size for the gradient descent step (here 200, but can explore!) and the loss function. The latter is the binary cross entropy, which is the standard choice for classification problems where we output a probability. It rewards \"confidence\" in a correct prediction (high probability). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssa1M_xP_Q00"
   },
   "source": [
    "The commands below can be used to explore possible choices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUYax3cN_Q00",
    "outputId": "a081df06-99db-462a-c893-cf05340654cd"
   },
   "source": [
    "dir(keras.optimizers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlvBNdKG_Q00",
    "outputId": "d605c371-d6ed-4346-b7f8-e9dd2725cfac"
   },
   "source": [
    "dir(keras.losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppQRvorFEKRc"
   },
   "source": [
    "A standard choice for a case like ours, where the labels are 0/1 but we can predict a probability, is the binary cross-entropy or log loss:\n",
    "\n",
    "L = - $\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot log(p(y_i)) + (1-y_i) \\cdot log (1 - p(y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpxfJMetEn6K"
   },
   "source": [
    "p is the probability that an object belongs to the positive class. It penalizes positive examples that are associated with predicted low probability, and negative examples that are associated with predicted high probability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wbZTVBd_Q00",
    "outputId": "7c7074c1-cf8c-4b8c-ce8f-f929562aa207"
   },
   "source": [
    "dir(keras.activations)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otwhTPnm_Q01"
   },
   "source": [
    "### This is how we build a fully connected neural network in keras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "woqxQqX1_Q01"
   },
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(20, activation='relu', input_shape=(24,)))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfWlB-0NA6CM"
   },
   "source": [
    "The \"metric\" keyword here serves to specify other possible metrics we would like to monitor. The loss itself is not interpretable, so we'll keep an eye on the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lkvPwRy_Q01"
   },
   "source": [
    "### Ready to fit?\n",
    "\n",
    "I hope so! Note also the additional hyperparameters \"epochs\" (the number of of back-and-forth passages), and batch size (how many of the data are used at every step in updating weights)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a2GD11m_Q01",
    "outputId": "8d20c752-baa6-449b-cc54-9737b64e46f1"
   },
   "source": [
    "mynet = model.fit(X_train, y_train, validation_data= (X_val, y_val), epochs = 100,  batch_size=200)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gSnZtIbBXgQ"
   },
   "source": [
    "This looks not so good."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "0rk8Pbdt_Q01",
    "outputId": "34c27aa4-cfa6-4f06-a8b3-958adbf3c0b8"
   },
   "source": [
    "plt.hist(model.predict(X_test), alpha = 0.5, label = 'pred')\n",
    "plt.hist(y_test, alpha = 0.5, label = 'true')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmdX7xQI_Q01"
   },
   "source": [
    "It's also helpful to plot the training and validation losses throughout the epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "ekaBUX5j_Q01",
    "outputId": "f9220253-629a-4cb3-edb9-aef4c4c73564"
   },
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('FirstNN.png', dpi= 300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJLdMkyp_Q01"
   },
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Based on the graphs above, how would you say this classifier is doing? Does it suffer from high variance or high bias?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The train and validation scores are close, so it's a high bias, not high variance problem. This is confirmed by the fact that the scores are really poor: around 70% accuracy, compared to the > 90% we obtained with SVMs.\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When something goes wrong, our first step should always be going back to the fundamentals of data exploration/setup."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "PzmnBG7__Q01",
    "outputId": "e22c579a-1370-4f62-d266-995afb18af56"
   },
   "source": [
    "X.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRv36Ye0tXMo"
   },
   "source": [
    "### Yep, we forgot scaling!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2qQMC772_Q02"
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T03xoxwA_Q02"
   },
   "source": [
    "scaler = StandardScaler()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Apply the scaler above to the correct sample.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "\n",
    "As usual, we only use the training set to derive the scaling! We need to run:\n",
    "\n",
    "```python\n",
    "scaler.fit(X_train)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the code from the learning checkin to proceed!\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the scaler that had been fit to transform the relevant data sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wLxWmWCk_Q03"
   },
   "source": [
    "Xst = scaler.transform(X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81RPh-OL_Q03",
    "outputId": "b661bb19-7cb2-48a3-9fcc-2dd8611c2a28"
   },
   "source": [
    "Xst.mean(axis=1) #Note: not exactly zero on the whole data set!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nczuqWMB_Q03"
   },
   "source": [
    "Xst_train = scaler.transform(X_train)\n",
    "Xst_val = scaler.transform(X_val)\n",
    "Xst_test = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIBbWjR9_Q03",
    "outputId": "db7e3bf8-7cd9-412b-cf04-21cadd3d3879"
   },
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=200)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "fjSveqjQ_Q03",
    "outputId": "e59a262d-a2a6-4e4f-e7f6-842fcd4a91a2"
   },
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ScaledNN.png', dpi= 300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "What is your assessment of the above classifier?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary><b>Click here for the answer!</b></summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The performance is now comparable to what we had obtained with SVMs. There are hints of high variance/overfitting, as shown by the gap between train and validation scores; it is hard to know how significant the gap is without a cross-validated approach. We can also see that the validation loss is increasing; this indicates that some regularization technique, such as early stopping and/or a Dropout layer, could help here.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zF4Dll3N_Q04"
   },
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(20, activation='relu', input_shape=(24,)))\n",
    "\n",
    "model.add(Dropout(0.2)) #This is the dropout fraction\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2)) #This is the dropout fraction\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) \n",
    "\n",
    "#The metric keyword here is for other possible metrics we would like to monitor "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5WAqdzI_Q04",
    "outputId": "976c7141-bd66-492a-e7ec-4d3c689b2daf"
   },
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=200)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "__hX7-bv_Q04",
    "outputId": "4104feff-c583-4096-c6b6-804271fda50a"
   },
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#plt.savefig('RegularizedNN.png', dpi= 300)\n",
    "#plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbWoRs0J_Q04",
    "outputId": "04cc32f5-2b2e-428e-e11c-a1db32c8b55b"
   },
   "source": [
    "# Final evaluation of the model (note this is done on the test set, so that if we do parameter optimization in the validation fold, this will be outside).\n",
    "\n",
    "scores = model.evaluate(Xst_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) #\"scores\" contains the test loss and the accuracy, which we are monitoring"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtWa7nxw_Q04",
    "outputId": "171a29c3-a602-40ff-8630-c963c215d8f6"
   },
   "source": [
    "scores"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FirstNN_Filled_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
