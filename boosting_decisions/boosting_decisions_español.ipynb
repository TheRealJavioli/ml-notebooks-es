{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGcAujuiugMm"
   },
   "source": [
    "### Boosting Decisions\n",
    "\n",
    "En este cuaderno, aprenderemos del proceso de \"boosting\" usando Boosting Adaptivo y los métodos de Árboles 'Boosted' con Gradiente. (Nota: \"boost\" aquí significa alzar o dar impulso, en el sentido de aumentar o empoderar algo.)\n",
    "\n",
    "La aplicación final será para el problema de corrimientos al rojo fotométricos del Capítulo 6; la solución particular para esos se verá en el cuaderno \"Tipos de Boosting.\"\n",
    "\n",
    "Autor: Viviana Acquaviva, con contibuciones de Jake Postiglione y Olga Privman. Traducido por Lucia Perez y Rosario Cecilio-Flores-Elie. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "14qv3IK8ugMn"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbFGJSw4ugMo"
   },
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uX1rgwqiugMo"
   },
   "source": [
    "Referencia que compara diferentes aprendices débiles cuando actúan como estimadores de base:\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-3-642-20042-7_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ud4yUdhugMo"
   },
   "source": [
    "Ejemplos de implementaciones empezando de casi nada (usando pesas de muestra):\n",
    "\n",
    "https://xavierbourretsicotte.github.io/AdaBoost.html\n",
    "\n",
    "\n",
    "https://geoffruddock.com/adaboost-from-scratch-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAVU67c3ugMp"
   },
   "source": [
    "### Primero imputamos en conjunto de los corrimientos al rojo fotométricos (ya con selecciones aplicadas) que creamos en el último cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "luvyhRXdugMp"
   },
   "source": [
    "sel_features = pd.read_csv('../data/sel_features.csv', sep = '\\t')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DdciikIbugMp"
   },
   "source": [
    "sel_target = pd.read_csv('../data/sel_target.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NeJbLw5vugMp"
   },
   "source": [
    "sel_features.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j1tVE11QugMp"
   },
   "source": [
    "sel_target.values.ravel() #cambia de forma a una matriz en forma de fila 1d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USVlKCvSugMp"
   },
   "source": [
    "### Podemos probar nuestro proceso de referencia con AdaBoost, usando los valores por defecto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uKz8mJvnugMp"
   },
   "source": [
    "model = AdaBoostRegressor()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5scFs3A-ugMp"
   },
   "source": [
    "ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GvwRTxLjugMp"
   },
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.ylim(0,3)\n",
    "plt.xlim(0,3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzSGa3UEugMp"
   },
   "source": [
    "### ¿Te parece que el proceso de !!!boosting está funcionando? A mi no."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gXypDDO6ugMp"
   },
   "source": [
    "model.get_params()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T7lW-PLugMp"
   },
   "source": [
    "Nota (de la documentacion de sklearn): Si \"None\", el estimador de base es \"DecisionTreeRegressor(max_depth=3)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-hhWg6sugMp"
   },
   "source": [
    "### Decidí investigar como diferentes parámetros afectan el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5mGuPYyugMq"
   },
   "source": [
    "#### Cambiar la profundidad máxima de los estimadores de base: intente usar árboles con no mas de 3, 6, o 10 partidas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "plxMoY2bugMq"
   },
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for i, depth in enumerate([3,6,10]):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=depth))\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, c = 'teal')\n",
    "    plt.title('Max depth = '+str(depth))\n",
    "    plt.xlabel('True redshift')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Estimated redshift')\n",
    "    plt.ylim(0,2)\n",
    "    plt.xlim(0,2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "#plt.savefig('AdaB_z.png')\n",
    "#    plt.axes('equal')\n",
    "#    plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8DwoQgVugMq"
   },
   "source": [
    "#### Cambiar en Número de estimadores de base (o, el número de etapas en el !!!boosting)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NOacuVRwugMq"
   },
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)\n",
    "\n",
    "for nest in [5,10,20]:\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), n_estimators=nest)\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, label = 'N est = '+str(nest))\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2vxWo4jugMq"
   },
   "source": [
    "#### Cambiar la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rd4X9WPQugMq"
   },
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)\n",
    "\n",
    "for loss in ['linear','square']:\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), loss = loss)\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, label = 'Loss = '+loss)\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg9uAtI3ugMq"
   },
   "source": [
    "### ¿Qué aprendimos? Para AdaBoost, el estimador de base necesita ser suficientemente 'fuerte' para tener éxito en !!!boosting ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRDR8BhXugMq"
   },
   "source": [
    "## Ejemplo básico de regresión\n",
    "### Inspirado por...\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlK2t1xkugMq"
   },
   "source": [
    "#### ¿Qué pasa si max_depth = 3?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wQBxJo8qugMq"
   },
   "source": [
    "# Creamos el conjunto de datos\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Encajamos el modelo de regresión, y guardamos cada 'etapa'\n",
    "\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predicción\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
    "    print('r2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
    "\n",
    "# Marcamos los resultados \n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "#plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"AdaBoost Regression, max depth = 3\", fontsize = 14)\n",
    "plt.legend(fontsize=10);\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"AdaBoost_3.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3CqYUSgugMq"
   },
   "source": [
    "#### ¿Qué pasa si max_depth = 6?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hgNtjtFwugMq"
   },
   "source": [
    "# Create the dataset\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=6)\n",
    "\n",
    "# Encajamos el modelo de regresión, y guardamos cada 'etapa'\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predicción\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
    "    print(metrics.r2_score(yp,y))\n",
    "\n",
    "# Marcamos los resultados \n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "#plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"AdaBoost Regression, max depth = 6\", fontsize = 14)\n",
    "plt.legend(fontsize=10);\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"AdaBoost_6.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhJzfhnXugMq"
   },
   "source": [
    "### Revisión de aprendizaje\n",
    "\n",
    "Mirando a la figura que creamos, ¿vale la pena hacer el proceso de boosting con AdaBoost si el aprendiz de base tiene max_depth = 3? ¿Vale la pena si max_depth = 6?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">¡Haga clic aquí para obtener la respuesta!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "No vale la pena para max_depth = 3--las notas de r2 no se mejoran si usamos mas estimadores. Tal vez valga la pena para max_depth = 6, pero las notas son estables, y por eso  tal vez no se necesita mas investigación. \n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHQt_g70ugMr"
   },
   "source": [
    "### Ahora que estamos convencidos, vamos a volver a los photo-zs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MFKXNqLugMr"
   },
   "source": [
    "Primero se crea una partida entre entrenamiento y prueba, para poder accesar la propiedad \"staged_predict\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TEd378nYugMr"
   },
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HHFAyLU9ugMr"
   },
   "source": [
    "# empezamos con un aprendiz muy débil (r2 = 0.4) \n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),\n",
    "                  n_estimators=30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n0bCjL6FugMr"
   },
   "source": [
    "model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovqvkDjKugMr"
   },
   "source": [
    "Podemos marcar la nota de r2 y el coeficiente de correlación Spearman entre los valores verdaderos y previstos, como una función del número de etapas/iteración, empezando con un aprendiz muy débil. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yMK8XeH9ugMr"
   },
   "source": [
    "n_estimators = 30\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.ylim(0,1.0)\n",
    "\n",
    "plt.title('Max depth = 3')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuJ8lLGAugMr"
   },
   "source": [
    "### Las notas no se mejoran con más estimadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWdoUpRUugMz"
   },
   "source": [
    "Intentemoslo de nuevo con un aprendiz de base más fuerte (max_depth = 6). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cpA5HzpFugMz"
   },
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 6')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n-FkvCxugMz"
   },
   "source": [
    "... y un aprendiz de base aún más fuerte (max_dpeth = 10)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pnBelFNvugMz"
   },
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=30)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2CH5bEpugMz"
   },
   "source": [
    "### Miremoslo todos en una figura."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7QCVqu-ugMz"
   },
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=md),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', AdaBoost')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('AdaB_performance.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8Q2424AugMz"
   },
   "source": [
    "### Revisión de aprendizaje\n",
    "\n",
    "Mirando a esta figura, ¿recomendarias usar AdaBoost con un aprendiz de base con max_depth = 6, y 30 iteraciones, o con max_depth = 10 y 10 iteraciones?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">¡Haga clic aquí para obtener la respuesta!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Las notas de r2 y la correlación entre los valores verdaderos y previstos ambas son más altas para el caso de max_depth = 10 y 10 iteraciones, así que esa es la mejor opción. \n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeYUoQ1zugMz"
   },
   "source": [
    "Tenemos una media-respuesta en el tercer panel de la figura, pero también se puede probar seguir con Boosting -- o, si agregar más etapas ayudará."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7arJUoY4ugMz"
   },
   "source": [
    "# ¿Seguimos con boosting? (max_depth = 10)\n",
    "\n",
    "n_estimators = 60\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHEdU0b_ugMz"
   },
   "source": [
    "### Conclusión: combinando aprendices que son demasiado débiles no ayuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaB-mLudugMz"
   },
   "source": [
    "### ¿Sería lo mismo para algoritmos de Árboles 'Boosted' con Gradiente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb7l9KjKugMz"
   },
   "source": [
    "¡Sabemos cómo averiguarlo!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vSwyPzK2ugM0"
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5VDCREfugM0"
   },
   "source": [
    "Los parámetros dependen de la implementación particular.\n",
    "\n",
    "En la formulación de sklearn, los parámetros de cada árbol son casi lo mismo a los que teníamos para los Bosques Aleatorios. Además tenemos el parámetro \"learning_rate\" (velocidad de aprendizaje), lo cual decide cuanto cada árbol contribuye al estimador final, y los parámetros de \"subsample\", los cuales nos dejan usar menos que 100% de las muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjZfJxeOugM0"
   },
   "source": [
    "Podemos ver como funciona con un aprendiz débil en nuestro conjunto del ejemplo básico."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "stbf3GiJugM0"
   },
   "source": [
    "# Crear el conjunto de datos\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Encajamos el modelo de regresión\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predicción \n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "y_100 = regr_100.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10, y_100]:\n",
    "    print('R2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
    "\n",
    "# Marcar los resultados.\n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "#plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.plot(X, y_100, \"-c\", label=\"n_estimators=100\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.title(\"Gradient Boosting Regression, max depth = 3\", fontsize = 14)\n",
    "plt.legend(fontsize=14, loc = 'upper right');\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"GradBoost_3.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RyeK6cHugM0"
   },
   "source": [
    "### Revisión de aprendizaje\n",
    "    \n",
    "¡Hay notas de r2 para unos GBTs (árboles 'boosted' con gradiente) que son negativos! ¿No deben las notas de r2 siempre ser positivas?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">¡Haga clic aquí para obtener la respuesta!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "No, ese requisito solo es para el conjunto de entrenamiento. Una nota negativa de r2 para el conjunto de prueba (o de validación) solo te dice que el desempeño del modélo es peor que una predicción constante equivalente al medio de la muestra. Seria un modelo horrible, pero no es necesariamente algo mal en nuestro código ( ;) ).\n",
    "\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yAcOCsWOugM0"
   },
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = GradientBoostingRegressor(max_depth=md,\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', GBR')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('GBR_performance.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aDry61eugM0"
   },
   "source": [
    "### Por causa del diferente proceso de boosting, los modelos de GBT funcionan bien hasta con aprendices de base débiles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmMX5VpOugM0"
   },
   "source": [
    "En el próximo cuaderno, compararemos AdaBoost y muchos modelos de GBT para el problema de corrimientos al rojo fotométricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY6b-XQ9ugM0"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
