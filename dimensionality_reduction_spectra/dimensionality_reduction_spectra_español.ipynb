{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un cuaderno simple para hacer ACP en espectros SDSS e imágenes de galaxias.\n",
    "\n",
    "Acompaña al Capítulo 7 del libro (2 de 4).\n",
    "\n",
    "Autora: Viviana Acquaviva, con contribuciones de Jake Postiglione y Olga Privman."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, decomposition\n",
    "%matplotlib inline\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage import io"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de componentes principales (ACP), o PCA por sus siglas en inglés, y algoritmos similares se utilizan para la reducción de la dimensionalidad en las ciencias con uso intensivo de datos.\n",
    "\n",
    "El objetivo principal del ACP lineal es encontrar las combinaciones lineales de características más representativas, de modo que cada elemento de un conjunto de datos pueda expresarse como la superposición (suma) de algunos vectores sobresalientes en el espacio de características (no es necesario que sean elementos de un conjunto de datos). En el ACP lineal más simple, los componentes principales son los vectores propios de la matriz de covarianza del conjunto de datos.\n",
    "\n",
    "Si el número de componentes es muy bajo (por ejemplo, 2 o 3, ACP u otros métodos de reducción de dimensionalidad permiten visualizar un conjunto de datos de alta dimensión como un gráfico 2D o 3D. Scikit-learn tiene métodos para calcular PCA y varias variantes. ACP Clásico tiene una complejidad difícil: $\\mathcal{O}[N^3].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos un ejemplo con espectros de galaxias de\n",
    "https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/dimensionality_reduction.html#sdss-spectral-data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = np.load('../data/spec4000_corrected.npz')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "wavelengths = data['wavelengths'] #wavelengts en inglés Longitud de onda\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "labels = data['labels'].astype('str')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X.shape # Forma de la matriz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "labels #No nos importan."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos graficar algunos ejemplos representativos de cada clase, solo para tener una idea de qué tipo de espectros hay en el conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i_class in (2, 3, 4, 5, 6):\n",
    "    i = np.where(y == i_class)[0][0]\n",
    "    l = plt.plot(wavelengths, X[i] + 20 * i_class)\n",
    "    c = l[0].get_color()\n",
    "    plt.text(6800, 2 + 20 * i_class, labels[i_class], color=c)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.xlabel('Longitud de onda (Angstroms)') \n",
    "plt.ylabel('flujo + constante')# Flujo\n",
    "plt.title('Muestra de Espectros'); # QS"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuestro conjunto de datos original tiene 4000 objetos y 1000 características.\n",
    "\n",
    "Intentaremos representarlo con una cantidad variable de componentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  Realizar ACP\n",
    "\n",
    "scaler = preprocessing.StandardScaler() #¡Es importante que los datos estén centrados!\n",
    "\n",
    "Xn = scaler.fit_transform(X) #Este es un procedimiento de estandarización.\n",
    "\n",
    "pca_50 = decomposition.PCA(n_components=50, random_state=0)# PCA por sus siglas en inglés\n",
    "\n",
    "pca_100 = decomposition.PCA(n_components=100, random_state=0)\n",
    "\n",
    "pca_1000 = decomposition.PCA(n_components=1000, random_state=0)\n",
    "\n",
    "X_proj_50 = pca_50.fit_transform(Xn) #el conjunto de datos proyectados: vive en un nuevo espacio de funciones con 4000 objetos y 50 características\n",
    "\n",
    "X_proj_100 = pca_100.fit_transform(Xn) #el conjunto de datos proyectados: vive en un nuevo espacio de funciones con 4000 objetos y 100 características\n",
    "\n",
    "X_proj_1000 = pca_1000.fit_transform(Xn) #el conjunto de datos proyectados: vive en un nuevo espacio de funciones con 4000 objetos y 1000 características\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registro de aprendizaje\n",
    "    \n",
    "¿Fue redundante el proceso anterior?\n",
    "<br>\n",
    "\n",
    "<details><summary><b>Haga clic aquí para la respuesta</b></summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "¡Sí! Los ACP se calculan de forma iterativa, siguiendo el mismo procedimiento, por lo que los primeros 50 componentes serán siempre los mismos, por muchos que generemos. Podríamos haber generado 1000 y luego mirar los primeros 50, 100 o 1000.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------------------\n",
    "#\n",
    "#  graficar espectros propios de PCA\n",
    "#\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "l = plt.plot(wavelengths, pca_50.mean_ - 0.15)\n",
    "c = l[0].get_color()\n",
    "plt.text(7000, -0.16, \"media\", color=c) \n",
    "\n",
    "# En ACP lineal, el primer vector propio es siempre la media, \n",
    "# y los primeros n componentes son siempre los mismos\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    l = plt.plot(wavelengths, pca_50.components_[i] + 0.15 * i)\n",
    "    \n",
    "    l = plt.plot(wavelengths, pca_100.components_[i] + 0.15 * i, linestyle = '-.')\n",
    "    \n",
    "    c = l[0].get_color()\n",
    "    \n",
    "    plt.text(7000, -0.01 + 0.15 * i, \"componente %i\" % (i + 1), color=c)\n",
    "\n",
    "    plt.ylim(-0.2, 0.6)\n",
    "    \n",
    "plt.xlabel('longitud de onda (Angstroms)')\n",
    "plt.ylabel('flujo escalado + constante')\n",
    "plt.title('Espectro medio y espectros propios')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos estimar la contribución de cada componente usando la propiedad \"razón de varianza explicada\".\n",
    "\n",
    "Estos son simplemente los valores propios de la matriz de covarianza. Su suma acumulada da la razón de varianza explicada (creciente progresivamente).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_50.explained_variance_ratio_ "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_1000.explained_variance_ratio_[-10:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar los vectores propios como la \"base\" que explica la mayor parte de la variabilidad de los datos.\n",
    "\n",
    "¿Cómo podemos saber si esto funciona? Hagamos ingeniería inversa del proceso:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Xrec_50 = pca_50.inverse_transform(X_proj_50) \n",
    "\n",
    "Xrec_100 = pca_100.inverse_transform(X_proj_100)\n",
    "\n",
    "Xrec_1000 = pca_1000.inverse_transform(X_proj_1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(4,8):\n",
    "    plt.subplot(2,2,i-3)\n",
    "    #plt.plot(wavelengths, Xn[i], label = 'orig', c = 'k')\n",
    "    #plt.plot(wavelengths, Xrec_50[i], '--', label = 'new, 50 PCs', c = 'g')\n",
    "    #plt.plot(wavelengths, Xrec_100[i], '--', label = 'new, 100 PCs', c = 'b')\n",
    "    #plt.plot(wavelengths, Xrec_1000[i], '--', label = 'new, 1000 PCs', c = 'r')\n",
    "    #plt.plot(wavelengths, (Xrec_50[i]-Xn[i])/Xn[i], '--', label = '% diff 50', c = 'g')\n",
    "    plt.plot(wavelengths, (Xrec_100[i]-Xn[i])/Xn[i], '--', label = 'diff 100', c = 'b')\n",
    "    #plt.plot(wavelengths, (Xrec_1000[i]-Xn[i])/Xn[i], '-.', label = 'diff 1000', c = 'k')\n",
    "    plt.ylim(-0.5,0.5)\n",
    "    plt.legend();\n",
    "plt.xlabel('longitud de onda (Angstroms)');\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registro de aprendizaje\n",
    "    \n",
    "¿Cómo espera que cambien los gráficos anteriores, si graficamos 1000 componentes en lugar de 100?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary><b>Haga clic aquí para la respuesta</b></summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    " La diferencia entre los espectros originales y el PCA con la misma cantidad de componentes que las entradas debe ser cero (o insignificante); en realidad, esta es una buena prueba de verificación.\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta: ¿cómo podemos saber cuál es un buen número de componentes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener una idea, podemos graficar la propiedad \"razón de varianza explicada\" ( en inglés \"explained_variance_ratio\") de la descomposición ACP. Se parece mucho al método del codo, pero al revés; en particular, la varianza explicada por N componentes siempre aumenta con N, pero suele haber un punto después del cual los rendimientos tienden a disminuir."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(np.cumsum(pca_1000.explained_variance_ratio_))\n",
    "plt.xlabel('número de componentes')\n",
    "plt.ylabel('varianza explicada acumulada');\n",
    "plt.xlim(0,20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registro de aprendizaje\n",
    "    \n",
    "¿Qué número de componentes recomendaría para el caso anterior?\n",
    "<br>\n",
    "\n",
    "<details><summary><b>Haga clic aquí para la respuesta</b></summary>\n",
    "<p>\n",
    "   \n",
    "    \n",
    "```\n",
    "Si solo nos basamos en la varianza explicada, parece que 5 o 10 componentes son suficientes, ¡pero esto puede NO ser lo suficientemente bueno para la ciencia que tenemos que hacer! La diferencia porcentual con los originales muestra que incluso para 50 o 100 componentes, todavía hay áreas (líneas de emisión/absorción, en particular) donde las diferencias son notables. Si son o no importantes realmente depende del caso de uso. En resumen: ¡No confíe únicamente en la varianza explicada!\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also:\n",
    "    \n",
    "https://arxiv.org/abs/2012.00066"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de ACP de Kernel"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_50 = decomposition.PCA(n_components=50, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "kpca_50 = decomposition.KernelPCA(n_components=50, \\\n",
    "                kernel = 'rbf', gamma = 0.2, eigen_solver = 'dense', \n",
    "                                  fit_inverse_transform = True, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_kproj_50 = kpca_50.fit_transform(Xn);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_proj_50.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_krec_50 = kpca_50.inverse_transform(X_kproj_50) #transformación más compleja"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(4,8):\n",
    "    plt.subplot(2,2,i-3)\n",
    "    #plt.plot(wavelengths, Xn[i], label = 'orig', c = 'k')\n",
    "    #plt.plot(wavelengths, X_krec_50[i], '--', label = 'new, 50 kPCs', c = 'g')\n",
    "    plt.plot(wavelengths, (X_krec_50[i]-Xn[i])/Xn[i], '--', label = '% diff k50', c = 'k')\n",
    "    plt.ylim(-0.1,0.1)\n",
    "    plt.legend()\n",
    "plt.xlabel('longitud de onda(Angstroms)');\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_kproj_50.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "kpca_50.lambdas_ #valores propios - Notese el cambio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "alphas = kpca_50.alphas_ #vectores propios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Comparar con esta implementación ( en inglés)\n",
    "#From https://sebastianraschka.com/Articles/2014_kernel_pca.html\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def stepwise_kpca(X, gamma, n_components):\n",
    "    \"\"\"\n",
    "    Implementación de un ACP de kernel PCA con una función de base radial (RBF por sus siglas en inglés).\n",
    "\n",
    "    Argumentos:\n",
    "        X: Un conjunto de datos MxN como matriz NumPy donde las muestras se almacenan como filas (M),\n",
    "           y los atributos definidos como columnas (N).\n",
    "        gamma: Un parámetro libre (coeficiente) para el kernel RBF.\n",
    "        n_components: El número de componentes a devolver.\n",
    "\n",
    "    Devuelve los k autovectores (alfas) que corresponden a los k más grandes valores propios (lambdas).      \n",
    "    \"\"\"\n",
    "    # Cálculo de las distancias euclidianas al cuadrado para cada par de puntos\n",
    "    # en el conjunto de datos dimensionales MxN.\n",
    "    sq_dists = pdist(X, 'sqeuclidean')\n",
    "\n",
    "    # Convertir las distancias por pares en una matriz MxM simétrica.\n",
    "    mat_sq_dists = squareform(sq_dists)\n",
    "\n",
    "    # Cálculo de la matriz kernel MxM.\n",
    "    K = np.exp(-gamma * mat_sq_dists)\n",
    "\n",
    "    # Centrado de la matriz kernel NxN simétrica.\n",
    "    N = K.shape[0]\n",
    "    one_n = np.ones((N,N)) / N\n",
    "    K_norm = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "\n",
    "    # Obtención de valores propios en orden descendente con su correspondiente\n",
    "    # vectores propios de la matriz simétrica.\n",
    "    eigvals, eigvecs = eigh(K_norm)\n",
    "\n",
    "    # Obtención de los i vectores propios (alfas) que corresponden a los i valores propios más altos (lambdas).\n",
    "    \n",
    "    alphas = np.column_stack((eigvecs[:,-i] for i in range(1,n_components+1)))\n",
    "    \n",
    "    lambdas = [eigvals[-i] for i in range(1,n_components+1)]\n",
    "\n",
    "    return alphas, lambdas"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora echemos un vistazo a las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos está compuesto por 200 imágenes seleccionadas aleatoriamente del desafío Kaggle Galaxy Zoo:\n",
    "\n",
    "https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge\n",
    "\n",
    "El siguiente código visualiza los primeros 25 objetos en su conjunto de datos. Puede ejecutarlo para obtener una vista de las primeras 25 galaxias. Nota: es posible que reciba un mensaje de error, en este caso, consulte aquí (en inglés):\n",
    "\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Takes <1 minuto\n",
    "\n",
    "images = []\n",
    "for i in range(200):\n",
    "    img =skimage.io.imread('../data/galaxy_images/Image_'+str(i)+'.png')\n",
    "    img_resized = resize(img,(100,100))\n",
    "    length = np.prod(img_resized.shape)\n",
    "    img_resized = np.reshape(img_resized,length)\n",
    "    images.append(img_resized)\n",
    "    \n",
    "images = np.vstack(images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(ncols= 5, nrows = 5,figsize=(50,50))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "\n",
    "    img = skimage.io.imread('../data/galaxy_images/Image_'+str(i)+'.png')\n",
    "    ax[i].imshow(img, cmap='gray')\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquí, hacemos la descomposición ACP en cada uno de los canales RGB (rojo, verde y azul) por separado. No estamos seguro de si es óptimo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r_images = images.reshape(200, -1,  3)[:,:,0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r_images.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Ejecutar ACP en las imágenes\n",
    "\n",
    "estimator = decomposition.PCA(n_components=100)\n",
    "\n",
    "r_images_PCA = estimator.fit_transform(r_images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Esto nos indica de la reducción de dimensionalidad que hemos hecho\n",
    "r_images_PCA.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "components = estimator.components_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos trazar los primeros 50 componentes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(12, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((estimator.components_[i].reshape(100, 100)), cmap='bone')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registro de aprendizaje\n",
    "    \n",
    "A partir de esta gráfica, ¿cuál creerías que es un número óptimo de componentes?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary><b>Haga clic aquí para la respuesta</b></summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Es difícil saberlo, pero quizás después de ~30 haya muy poca estructura en las imágenes propias.```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la relación de varianza explicada para ver si hay un número óptimo obvio de componentes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(np.cumsum(estimator.explained_variance_ratio_))\n",
    "plt.xlabel('número de componentes')\n",
    "plt.ylabel('varianza explicada acumulada');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión: de nuevo, no es obvio, pero tal vez 25-30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruyamos ahora las imágenes originales."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r_projected = estimator.inverse_transform(r_images_PCA)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(15, 5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(r_images[i].reshape(100, 100), cmap='gray')\n",
    "    ax[1, i].imshow(r_projected[i].reshape(100, 100), cmap='gray')\n",
    "#diff    ax[1, i].imshow((r_projected[i] - r_images[i]).reshape(100, 100), cmap='gray')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacerlo para los tres canales a la vez y luego unirlos:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "estimator = decomposition.PCA(n_components=100) \n",
    "\n",
    "r_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(r_images)\n",
    "r_images_PCA = estimator.fit_transform(r_images)\n",
    "r_projected = estimator.inverse_transform(r_images_PCA)\n",
    "\n",
    "g_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(g_images)\n",
    "g_images_PCA = estimator.fit_transform(g_images)\n",
    "g_projected = estimator.inverse_transform(g_images_PCA)\n",
    "\n",
    "b_images = images.reshape(200, -1,  3)[:,:,2]                     \n",
    "estimator.fit(b_images)\n",
    "b_images_PCA = estimator.fit_transform(b_images)\n",
    "b_projected = estimator.inverse_transform(b_images_PCA)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 5, figsize=(50, 20),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(5):\n",
    "    ax[0, i].imshow((np.dstack([r_images[i].reshape(100, 100)*255, g_images[i].reshape(100, 100)*255, \n",
    "        b_images[i].reshape(100,100)*255]).astype(np.uint8)))\n",
    "    ax[1, i].imshow((np.dstack([r_projected[i].reshape(100, 100)*255, g_projected[i].reshape(100, 100)*255, \n",
    "        b_projected[i].reshape(100,100)*255]).astype(np.uint8)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las técnicas de reducción de la dimensionalidad son útiles tanto para desarrollar la comprensión de lo que hay en los datos como para hacer que los tamaños sean más manejables.\n",
    "\n",
    "¡El agrupamiento y la reducción de dimensionalidad se superponen mucho! Por ejemplo en el Ejemplo # 2 de:\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n",
    "\n",
    "#### Las técnicas no lineal (t-SNE, mapas autoorganizados...) son herramientas populares para la visualización en el espacio 2D y son útiles para la exploración/investigación de datos. Sin embargo, tienen parámetros ajustables que no son fáciles de ajustar y son difíciles de interpretar.\n",
    "\n",
    "#### Feliz de seleccionar algunas referencias/material de lectura si alguien está interesado.\n",
    "\n",
    "Por ejemplo en inglés:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html#manifold\n",
    "\n",
    "o\n",
    "\n",
    "https://www.superdatascience.com/blogs/the-ultimate-guide-to-self-organizing-maps-soms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
